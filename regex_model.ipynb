{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "class DDBAPI_GET_XMLS:\n",
        "    def __init__(self):\n",
        "        self.api_key = \"OYSi9Dygc0XZ0Nvq2vgPxe4oXNmomCtWWZHM7CVd3Fo7iC0qKge1748029090188\"  \n",
        "        self.headers = {\n",
        "            \"Authorization\": f'OAuth oauth_consumer_key=\"{self.api_key}\"',\n",
        "            \"Accept\": \"application/json\",\n",
        "        }\n",
        "    \n",
        "    def safe_get(self, url, headers=None, retries=3, backoff=5, timeout=20):\n",
        "        \"\"\"\n",
        "        Wrapper around requests.get with retries, timeout, and error handling.\n",
        "        \"\"\"\n",
        "        for attempt in range(1, retries + 1):\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, timeout=timeout)\n",
        "                response.raise_for_status()  # raise HTTPError for bad responses\n",
        "                return response\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"‚è≥ Timeout on attempt {attempt}/{retries} for {url}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"‚ö†Ô∏è Request failed on attempt {attempt}/{retries}: {e}\")\n",
        "            \n",
        "            if attempt < retries:\n",
        "                print(f\"üîÅ Retrying in {backoff} seconds...\")\n",
        "                time.sleep(backoff)\n",
        "\n",
        "        print(f\"‚ùå Failed to fetch {url} after {retries} attempts.\")\n",
        "        return None\n",
        "\n",
        "    def get_xmls_only(self, item_id, base_dir=\"ddb\"):\n",
        "        item_url = f\"https://api.deutsche-digitale-bibliothek.de/items/{item_id}\"\n",
        "        response = self.safe_get(item_url, headers=self.headers)\n",
        "        if not response or response.status_code != 200:\n",
        "            print(f\"‚ùå Failed to fetch item {item_id}\")\n",
        "            return None, 0, '', '', ''\n",
        "\n",
        "        try:\n",
        "            data = response.json()\n",
        "            mets_xml = data[\"source\"][\"record\"][\"$\"]\n",
        "            issued = data['edm']['RDF']['ProvidedCHO']['issued']\n",
        "            publisher = data['edm']['RDF']['ProvidedCHO']['publisher']['$']\n",
        "            title = data['edm']['RDF']['ProvidedCHO']['title']['$']\n",
        "            xml_links = re.findall(r'https://[^\\s\"]+\\.xml', mets_xml)\n",
        "            xml_links = [url for url in xml_links if url.startswith(\"https://api.deutsche-digitale-bibliothek.de/binary/\")]\n",
        "\n",
        "            # Create output folder\n",
        "            folder = os.path.join(base_dir, item_id)\n",
        "            os.makedirs(folder, exist_ok=True)\n",
        "            print(f\"\\nüì• Downloading XMLs to: {folder}\")\n",
        "\n",
        "            for i, xml_url in enumerate(xml_links, 1):\n",
        "                xml_resp = self.safe_get(xml_url, timeout=30)\n",
        "                if xml_resp and xml_resp.status_code == 200:\n",
        "                    xml_path = os.path.join(folder, f\"page_{i}.xml\")\n",
        "                    with open(xml_path, \"wb\") as f:\n",
        "                        f.write(xml_resp.content)\n",
        "                    print(f\"‚úî Saved {xml_path}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to download XML: {xml_url}\")\n",
        "\n",
        "            return folder, len(xml_links), issued, publisher, title\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùó Error with item {item_id}: {e}\")\n",
        "            return None, 0, '', '', ''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def xml2text(xml_path):\n",
        "    # Parse the XML file\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Dynamically get the namespace from the root tag\n",
        "    if root.tag.startswith(\"{\"):\n",
        "        ns_uri = root.tag[1:root.tag.find(\"}\")]\n",
        "        ns = {'alto': ns_uri}\n",
        "    else:\n",
        "        ns = {'alto': ''}  # fallback if no namespace is defined\n",
        "\n",
        "    lines = []\n",
        "\n",
        "    # Traverse each TextLine in the XML\n",
        "    for text_line in root.findall('.//alto:TextLine', ns):\n",
        "        words = [string.attrib.get(\"CONTENT\", \"\") for string in text_line.findall(\"alto:String\", ns)]\n",
        "        line_text = \" \".join(words)\n",
        "        if line_text.strip():\n",
        "            lines.append(line_text)\n",
        "\n",
        "    full_text = \"\\n\".join(lines)\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# **** |id|date|page number|chunk|response| ****\n",
        "import csv\n",
        "import os\n",
        "\n",
        "class DatasetCollector:\n",
        "    def __init__(self, csv_file):\n",
        "        self.csv_file = csv_file\n",
        "\n",
        "\n",
        "    def add_row(self, row_dict):\n",
        "        # Create the CSV file and write the header if it doesn't exist\n",
        "        if not os.path.exists(self.csv_file):\n",
        "            with open(self.csv_file, mode='w', encoding='utf-8', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"item_id\", \"publisher\", \"title\", \"pub_date\", \"page_num\", \"chunk\", \"success\", \"anarch\", \"terror\", \"kommunis\", \"sozial\", \"revolut\"])\n",
        "        with open(self.csv_file, mode='a', encoding='utf-8', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=row_dict.keys())\n",
        "            writer.writerow(row_dict)\n",
        "\n",
        "    def add_ids_only(self, id_list):\n",
        "        \"\"\"\n",
        "        Appends a list of IDs into a CSV file, one per row in a single column.\n",
        "        If the file doesn't exist, it creates it with a header.\n",
        "        \"\"\"\n",
        "        file_exists = os.path.isfile(self.csv_file)\n",
        "\n",
        "        try:\n",
        "            with open(self.csv_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "\n",
        "                # Write header if file is new\n",
        "                if not file_exists:\n",
        "                    writer.writerow([\"id\"])\n",
        "\n",
        "                # Write each ID\n",
        "                for item_id in id_list:\n",
        "                    writer.writerow([item_id])\n",
        "\n",
        "            print(f\"‚úî Added {len(id_list)} IDs to {self.csv_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error writing IDs to {self.csv_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Function to clean the folder by deleting all files and subfolders\n",
        "def clean_the_folder(base_dir=\"ddb\"):\n",
        "    \"\"\"\n",
        "    Deletes all files and subfolders in the given base_dir.\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(base_dir):\n",
        "        shutil.rmtree(base_dir)\n",
        "        print(f\"Deleted all contents in '{base_dir}'\")\n",
        "    else:\n",
        "        print(f\"Directory '{base_dir}' does not exist.\")\n",
        "\n",
        "# Function to chunk text into smaller parts for LLM processing\n",
        "def chunk_text_by_words(text, max_words):\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), max_words):\n",
        "        yield ' '.join(words[i:i + max_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "terms = [\"anarch\", \"terror\", \"kommunis\", \"sozial\", \"revolut\"]\n",
        "zdb = \"2764651-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "my_name = \"4_Bakhadir\"\n",
        "\n",
        "# df = pd.read_csv(f\"./BerTagHanZeitung/ids_split_{my_name}_1.csv\")\n",
        "# df = pd.read_csv(f\"./BerTagHanZeitung/ids_split_{my_name}_2.csv\")\n",
        "# df = pd.read_csv(f\"./BerTagHanZeitung/ids_split_{my_name}_3.csv\")\n",
        "df = pd.read_csv(f\"./BerTagHanZeitung/ids_split_{my_name}_4.csv\")\n",
        "\n",
        "id_list = df['id'].tolist()\n",
        "total_ids = len(id_list)\n",
        "print(total_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from regex_search_model import RegexSearchModel\n",
        "model = RegexSearchModel(terms)\n",
        "\n",
        "os.makedirs(\"./BerTagHanZeitung/outdata\", exist_ok=True)\n",
        "\n",
        "# csv_file = f\"./BerTagHanZeitung/outdata/output_{my_name}_1.csv\"\n",
        "# csv_file = f\"./BerTagHanZeitung/outdata/output_{my_name}_2.csv\"\n",
        "# csv_file = f\"./BerTagHanZeitung/outdata/output_{my_name}_3.csv\"\n",
        "csv_file = f\"./BerTagHanZeitung/outdata/output_{my_name}_4.csv\"\n",
        "collector = DatasetCollector(csv_file)\n",
        "ddb = DDBAPI_GET_XMLS()\n",
        "counter = 0\n",
        "for item_id in id_list: \n",
        "    counter += 1\n",
        "    print(f\"\\nProcessing item {counter}/{total_ids}: {item_id}\")\n",
        "    folder, numpages, issued, publisher, title = ddb.get_xmls_only(item_id)\n",
        "    \n",
        "    if folder == None or numpages == 0:\n",
        "        print(f\"Skipping item {item_id} due to a download error.\")\n",
        "        continue\n",
        "    print(f\"Processing item {item_id} with {numpages} pages.\")\n",
        "    for page in range(1, numpages + 1):\n",
        "        xml_path = os.path.join(folder, f\"page_{page}.xml\")\n",
        "\n",
        "        if not os.path.exists(xml_path):\n",
        "            print(f\"Skipping missing files for item {item_id}, page {page}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            full_text = xml2text(xml_path)\n",
        "        except (ET.ParseError, FileNotFoundError) as e:\n",
        "            print(f\"‚ö†Ô∏è Could not parse XML {xml_path}: {e}\")\n",
        "            continue   # skip this page if XML is malformed\n",
        "        # start processing the text with LLM by chunking it into smaller parts\n",
        "        # to avoid exceeding the token limit\n",
        "        max_chunk_length = 100 \n",
        "\n",
        "        for idx, chunk in enumerate(chunk_text_by_words(full_text, max_chunk_length), 1):\n",
        "            print(f\"Processing item {item_id}, {counter}, chunk {idx}\")\n",
        "            model_response = model.generate_response(chunk)\n",
        "            # If model_response is not a dict (e.g., JSON decode failed), wrap it\n",
        "            collector.add_row({\n",
        "                \"item_id\": item_id,\n",
        "                \"publisher\": publisher,\n",
        "                \"title\": title,\n",
        "                \"pub_date\": issued,\n",
        "                \"page_num\": page,\n",
        "                \"chunk\": chunk,\n",
        "                **model_response  # if model_response is a dict\n",
        "            })\n",
        "    \n",
        "    # delete the image and xml files\n",
        "    clean_the_folder()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
